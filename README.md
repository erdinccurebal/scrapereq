# ğŸ•¸ï¸ Scrapereq

<div align="center">
  <h3>A powerful and flexible web scraping API built with Express.js and Puppeteer</h3>
  <p>
    <img src="https://img.shields.io/badge/Express-5.1.0-000000?style=flat-square&logo=express" alt="Express.js" />
    <img src="https://img.shields.io/badge/Puppeteer-24.8.0-40B5A4?style=flat-square&logo=puppeteer" alt="Puppeteer" />
    <img src="https://img.shields.io/badge/Node.js-v20+-339933?style=flat-square&logo=node.js" alt="Node.js" />
    <img src="https://img.shields.io/badge/License-ISC-blue?style=flat-square" alt="License" />
    <img src="https://img.shields.io/github/actions/workflow/status/erdinccurebal/scrapereq/ci.yml?branch=master&style=flat-square&label=CI" alt="CI" />
  </p>
  <p>
    <a href="#-api-endpoints">API Documentation</a> â€¢
    <a href="#-installation">Installation</a> â€¢
    <a href="#-features">Features</a> â€¢
    <a href="#-contributing">Contributing</a>
  </p>
</div>

## ğŸ“‹ Overview

Scrapereq is a RESTful API service that allows you to perform web scraping operations by defining a series of steps executed by a headless browser. It provides a clean and secure way to extract data from websites with advanced features like proxy support, customizable scraping speeds, robust validation, and error handling.

## âœ¨ Features

### Core Capabilities

- **ğŸ”„ Step-Based Scraping**: Define your scraping workflow as a series of steps (navigate, click, wait, setViewport, etc.)
- **âš¡ Speed Control**: Multiple speed modes (TURBO, FAST, NORMAL, SLOW, SLOWEST, CRAWL, STEALTH)
- **ğŸ” Selector Support**: Extract data using CSS, XPath, or full page HTML selectors
- **âœ… Enhanced Validation**: Comprehensive request validation with clear error messages

### Security & Reliability

- **ğŸ” Built-in Security**: Basic authentication, helmet protection, and CORS configuration
- **ğŸŒ Enhanced Proxy Support**: Advanced proxy configuration with authentication and multiple proxy rotation
- **ğŸ›¡ï¸ Error Handling**: Consistent JSON error responses with contextual details and optional stack traces for debugging
- **ğŸ’ª Browser Resilience**: Automatic disconnection detection and resource management

### Advanced Features

- **ğŸ“¸ Screenshot Capabilities**: Capture success and error screenshots with configurable options
- **ğŸ“Š API Monitoring**: Detailed health check endpoint with system information
- **ğŸ“ Swagger Documentation**: Interactive API documentation with detailed request/response examples
- **ğŸ”§ System Controls**: Application shutdown and OS restart endpoints
- **ğŸ’¾ Persistent Storage**: Configurable screenshot directory for persistent storage across deployments
- **ğŸ§¹ Automatic Cleanup**: Automated cleanup of old screenshot files
- **ğŸ“ˆ Performance Metrics**: Track and analyze scraping performance with detailed metrics
- **ğŸ” Retry Mechanism**: Intelligent retry functionality for handling transient errors
- **ğŸ› ï¸ CLI Utilities**: User-friendly command-line interface for development and deployment

## ğŸ› ï¸ Tech Stack

- **ğŸ“¦ Node.js**: JavaScript runtime
- **ğŸš€ Express.js v5.1.0**: Web application framework
- **ğŸ¤– Puppeteer v24.8.0**: Headless Chrome browser automation
- **ğŸ§© Puppeteer-Extra v3.3.6**: Plugin system for Puppeteer
- **âºï¸ @puppeteer/replay v3.1.1**: Record and replay browser interactions
- **âœ… Joi v17.13.3**: Request validation
- **ğŸ“ Morgan**: HTTP request logging
- **ğŸ›¡ï¸ Helmet v8.1.0**: Security middleware
- **ğŸ“š Swagger-JSDoc v6.2.8**: API documentation generation
- **ğŸŒ Swagger-UI-Express v5.0.1**: Interactive API documentation
- **ğŸŒ CORS**: Cross-Origin Resource Sharing support
- **âš™ï¸ dotenv v16.5.0**: Environment configuration
- **ğŸ§ª Jest & Supertest**: Testing framework

## ğŸš€ Installation

1. **Clone the repository**:

   ```bash
   git clone https://github.com/erdinccurebal/scrapereq.git
   cd scrapereq
   ```

2. **Install dependencies**:

   ```bash
   npm install
   ```

3. **Create a configuration file**:

   Create a `.env` file in the root directory based on the following template:

   ```env
   # Server Configuration
   PORT=3000
   HOST=localhost
   NODE_ENV=development
   WEB_ADDRESS=http://localhost:3000

   # Authentication
   AUTH_USERNAME=admin
   AUTH_PASSWORD=secretpassword

   # Puppeteer Configuration
   CHROME_PATH=/path/to/chrome # Optional custom Chrome path

   # File Storage
   TMP_DIR=/path/to/persistent/directory # Optional: defaults to ./tmp

   # Browser Concurrency
   MAX_CONCURRENT_BROWSERS=2 # Number of concurrent browser instances

   # Rate Limiting
   RATE_LIMIT_WINDOW_MS=900000 # 15 minutes in milliseconds
   RATE_LIMIT_MAX_REQUESTS=100 # Maximum requests per window

   # Proxy Configuration (Optional)
   SCRAPE_PROXY_BYPASS_CODE=your_secure_password # Password to bypass proxy requirement
   ```

4. **Start the application**:
   ```bash
   npm start
   ```

## ğŸ³ Docker Deployment

You can easily run the application using Docker:

```bash
# Build the Docker image
npm run docker:build

# Run the container
npm run docker:run
```

Or use the provided docker-compose.yml:

```bash
docker-compose up -d
```

## ğŸ”Œ API Endpoints

The API provides the following main endpoints:

### ğŸ” Health Check

```http
GET /api/app/health
```

Returns detailed system information and checks if all components are working correctly.

### ğŸ•¸ï¸ Scraper

```http
POST /api/scrape/start
```

Main endpoint for web scraping operations. Configure your scraping workflow with a detailed JSON structure.

#### Example Request:

<details>
<summary>ğŸ“‹ View example request body</summary>

```json
{
  "proxy": {
    "bypassCode": "your_secure_password",
    "auth": {
      "enabled": true,
      "username": "proxyuser",
      "password": "proxypass"
    },
    "servers": [
      {
        "server": "proxy1.example.com",
        "port": 8080
      },
      {
        "server": "proxy2.example.com",
        "port": 8081
      }
    ]
  },
  "record": {
    "title": "Google Search Example",
    "speedMode": "NORMAL",
    "timeoutMode": "NORMAL",
    "steps": [
      {
        "type": "navigate",
        "url": "https://www.google.com"
      },
      {
        "type": "wait",
        "value": "1000"
      },
      {
        "type": "setViewport",
        "width": 1366,
        "height": 768
      },
      {
        "type": "click",
        "selectors": [["#L2AGLb"]]
      },
      {
        "type": "change",
        "selectors": [["input[name='q']"]],
        "value": "web scraping api"
      },
      {
        "type": "click",
        "selectors": [["input[name='btnK']"]]
      },
      {
        "type": "waitForElement",
        "selectors": [["#search"]]
      }
    ]
  },
  "capture": {
    "selectors": [
      {
        "key": "search_results",
        "type": "CSS",
        "value": "#search"
      },
      {
        "key": "page_title",
        "type": "CSS",
        "value": "title"
      }
    ]
  },
  "headers": {
    "Accept-Language": "en-US,en;q=0.9",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36"
  },
  "output": {
    "screenshots": {
      "onError": true,
      "onSuccess": true
    },
    "responseType": "JSON"
  }
}
```

</details>

### ğŸ§ª Test Endpoint

```http
POST /api/scrape/test
```

Runs a predefined scraping test using a fixed configuration. This endpoint is useful for:

- Testing if the scraping service is working correctly
- Checking proxy connectivity
- Validating browser functionality

The test endpoint uses a predefined configuration from `constants.js` with a sample scrape request that checks your IP address using a proxied connection.

### ğŸ”§ System Management

```http
POST /api/app/shutdown
```

Safely shuts down the application.

```http
POST /api/os/restart
```

Initiates an operating system restart (requires appropriate permissions).

## ğŸ“š Documentation

For complete API documentation, visit the Swagger UI endpoint after starting the application:

```
http://localhost:3000/api/docs
```

## ğŸ” Selector Types

Data can be extracted using different selector methods:

| Selector Type | Usage                                |
| ------------- | ------------------------------------ |
| `CSS`         | Standard CSS selectors               |
| `XPATH`       | XPath expressions                    |
| `FULL`        | Retrieves the full page HTML content |

## ğŸ”„ Response Types

The scraper supports multiple response formats:

| Type   | Description                                          |
| ------ | ---------------------------------------------------- |
| `JSON` | Returns structured JSON with data and metadata       |
| `RAW`  | Returns raw content from the first selector          |
| `NONE` | No response content (useful for headless operations) |

## âš ï¸ Error Handling

The API implements a consistent error handling pattern:

- **Standardized Format**: All errors return a consistent JSON structure
- **Contextual Information**: Includes error code, message, and related data
- **Debug Support**: Stack traces included in development mode
- **Visual Evidence**: Error screenshots for visual debugging
- **Step Identification**: Clear indication of which step in the process failed
- **Proxy Errors**: Detailed information about proxy-related issues

Example error response:

```json
{
  "success": false,
  "data": {
    "message": "Failed to execute click operation on element",
    "code": "ERROR_ELEMENT_NOT_FOUND",
    "stepIndex": 3,
    "screenshotUrl": "/tmp/error-screenshot-123456.png"
  }
}
```

## ğŸ› ï¸ CLI Startup Options

The project includes several command-line utility scripts:

```bash
# Start the application
npm start

# Run tests
npm test

# Lint code
npm run lint

# Fix linting issues
npm run lint:fix

# Format code with Prettier
npm run format

# Docker operations
npm run docker:build   # Build Docker image
npm run docker:run     # Run Docker container
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ index.js                # Entry point
â”œâ”€â”€ docker-compose.yml      # Docker Compose configuration
â”œâ”€â”€ Dockerfile              # Docker configuration
â”œâ”€â”€ src/                    # Application source code
â”‚   â”œâ”€â”€ app.js              # Express app configuration
â”‚   â”œâ”€â”€ config.js           # Configuration module
â”‚   â”œâ”€â”€ constants.js        # Constants and enums
â”‚   â”œâ”€â”€ controllers/        # Request handlers
â”‚   â”‚   â”œâ”€â”€ error-handler.js # Global error handling middleware
â”‚   â”‚   â””â”€â”€ api/            # API controllers
â”‚   â”œâ”€â”€ helpers/            # Helper functions
â”‚   â”‚   â”œâ”€â”€ browser-semaphore.js   # Browser instance management
â”‚   â”‚   â”œâ”€â”€ cleanup-screenshots.js # Screenshot cleanup utility
â”‚   â”‚   â”œâ”€â”€ do-scraping.js         # Main scraping logic
â”‚   â”‚   â”œâ”€â”€ proxies-random-get-one.js # Proxy rotation utility
â”‚   â”‚   â”œâ”€â”€ scrape-validate-req-body.js # Request validation
â”‚   â”‚   â””â”€â”€ validators.js          # Schema validation definitions
â”‚   â”œâ”€â”€ routes/             # API route definitions
â”‚   â””â”€â”€ utils/              # Utility middleware
â”œâ”€â”€ __tests__/              # Test files
â””â”€â”€ tmp/                    # Temporary files directory
```

## ğŸ¤ Contributing

Contributions are welcome! Please read our [Contributing Guide](CONTRIBUTING.md) before submitting a Pull Request.

By participating in this project, you agree to abide by our [Code of Conduct](CODE_OF_CONDUCT.md).

## ğŸ”’ Security

For security vulnerabilities, please see our [Security Policy](SECURITY.md). Do **not** open a public issue for security concerns.

## ğŸ“„ License

This project is licensed under the ISC License - see the [LICENSE](LICENSE) file for details.
